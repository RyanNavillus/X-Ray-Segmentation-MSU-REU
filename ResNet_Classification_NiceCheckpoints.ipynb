{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Detection in Chest Radiographs\n",
    "\n",
    "The goal of this notebook is to use pretrained ImageNet models and transfer learning to identify lines in . medical images. These lines can be catheters, tubes, or many other types of medical support devices. We are using the [Stanford CheXpert dataset](https://stanfordmlgroup.github.io/competitions/chexpert/) for training and validation. \n",
    "\n",
    "This is a binary classification problem. The network will simply determine if an X-ray image contains or does not contain any medical lines with the classes `Line` and `NoLine` respectively.\n",
    "\n",
    "The end goal is to be able to use the weights in this network as a backbone for a more complex medical image segmentation network such as U-Net or Mask R-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 22:11:17) \n",
      "[GCC 7.3.0]\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import keras\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "print(sys.version)\n",
    "print(tf.__version__)\n",
    "%matplotlib inline\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "The names of the images with lines, and those without lines, are stored in a two separate text files. We will need to read these text files, load the images, then split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input image size\n",
    "input_size = (512, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the image list files**\n",
    "\n",
    "Load the file containing paths to the line images, and the file containing paths to the noline images. (Loading images may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = \"/data/ryan_data/CheXpert/CheXpert-v1.0/train/\" # Path to CheXpert dataset train/val folder \n",
    "base_path = \"../Data/CheXpert-sorted/train/\" # Path to CheXpert dataset train/val folder \n",
    "line_path = \"./line.txt\"\n",
    "noline_path = \"./noline.txt\"\n",
    "\n",
    "def load_image_cv(path):\n",
    "    \"\"\"Load grayscale image from path\"\"\"\n",
    "    return cv2.imread(path)\n",
    "\n",
    "def load_samples(file):\n",
    "    images = []\n",
    "    for image_path in file:\n",
    "        path = os.path.join(base_path, image_path.rstrip())\n",
    "        images.append(cv2.resize(load_image_cv(path), input_size))\n",
    "    return np.asarray(images)\n",
    "\n",
    "line_file = open(line_path, mode=\"r\", encoding=\"utf-8\")\n",
    "line_images = load_samples(line_file)\n",
    "\n",
    "noline_file = open(noline_path, mode=\"r\", encoding=\"utf-8\")\n",
    "noline_images = load_samples(noline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an image of each class to check that the dataset was loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 14))\n",
    "ax[0].imshow(line_images[11], cmap=\"gray\")\n",
    "ax[0].set_title(\"Line\")\n",
    "ax[1].imshow(noline_images[11], cmap=\"gray\")\n",
    "ax[1].set_title(\"Noline\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of line images: {}\".format(len(line_images)))\n",
    "print(\"Number of noline images: {}\".format(len(noline_images)))\n",
    "print(\"Shape of each line sample: {}\".format(line_images[0].shape))\n",
    "print(\"Shape of each noline sample: {}\".format(noline_images[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine Data and Labels**\n",
    "\n",
    "Since our data is intially split into separate lists by class, we need to rearrange them into two ordered lists of sample images and their labels. We use the following binary labels:\n",
    "* 0 = noline\n",
    "* 1 = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class split:  74.79% line,  25.21% noline\n",
      "Samples shape: (2999, 512, 512, 3)\n",
      "Labels shape: (2999, 2)\n"
     ]
    }
   ],
   "source": [
    "# If necessary, some methods may be included to improve the class split\n",
    "total_samples = len(line_images) + len(noline_images)\n",
    "line_percent = len(line_images) / total_samples * 100\n",
    "noline_percent = len(noline_images) / total_samples * 100\n",
    "print(\"Class split: {0: .2f}% line, {1: .2f}% noline\".format(line_percent, noline_percent))\n",
    "\n",
    "# Combine data into samples and labels arrays\n",
    "samples = np.asarray([x for x in line_images] + [x for x in noline_images])\n",
    "labels = np.asarray([(1,0) for x in line_images] + [(0,1) for x in noline_images])\n",
    "\n",
    "samples = np.reshape(samples, (len(samples), input_size[0], input_size[1], 3))\n",
    "\n",
    "# Print sample and label shapes\n",
    "print(\"Samples shape: {}\".format(samples.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train/test/val sets**\n",
    "\n",
    "Split the data into individual sets. Here we use a 70/20/10 split for training, validation, and testing respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length:\t2099\n",
      "Test Length:\t599\n",
      "Val Length:\t301\n"
     ]
    }
   ],
   "source": [
    "# Randomize dataset ordering\n",
    "ordering = np.arange(len(samples))\n",
    "np.random.shuffle(ordering)\n",
    "samples = samples[ordering]\n",
    "labels = labels[ordering]\n",
    "\n",
    "\n",
    "train_split = 0.7\n",
    "val_split = 0.2\n",
    "test_split = 0.1\n",
    "n = len(samples)\n",
    "\n",
    "# Split data\n",
    "train_cutoff = int(n * train_split)\n",
    "val_cutoff = int(train_cutoff + (n * val_split))\n",
    "X_train, X_val, X_test = samples[:train_cutoff], samples[train_cutoff:val_cutoff], samples[val_cutoff:]\n",
    "Y_train, Y_val, Y_test = labels[:train_cutoff], labels[train_cutoff:val_cutoff], labels[val_cutoff:]\n",
    "\n",
    "print(\"Train length:\\t{}\\nTest Length:\\t{}\\nVal Length:\\t{}\".format(len(X_train), len(X_val), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.01\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create base model**\n",
    "\n",
    "We initialize the ResNet50 model, removing the final classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/midi-lab/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create the ResNet50 base model Dense(1000) layers\n",
    "# Docs: https://keras.rstudio.com/reference/application_resnet50.html\n",
    "resnet_base = ResNet50(weights=\"imagenet\", input_shape=(input_size[0], input_size[1], 3), include_top=False, pooling=\"avg\")\n",
    "\n",
    "# Uncomment for the base model summary\n",
    "#resnet_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add classification layers to ResNet**\n",
    "\n",
    "We add two dense layers and an output layer to provide some room for the network to map CNN features to our outputs. Since this is a binary classification problem, we will use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras functional programming syntax: https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "# Add 2 dense layers and a final prediction layer\n",
    "top = Dense(1024, activation=\"relu\")(resnet_base.output) # TEST: kernel_regularizer = regularizers.l2(0.05)\n",
    "top = Dense(256, activation=\"relu\")(top)\n",
    "predictions = Dense(2, activation=\"softmax\")(top)\n",
    "model = Model(inputs=resnet_base.input, outputs=predictions)\n",
    "\n",
    "# Uncomment for full model summary\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose the trainable layers**\n",
    "\n",
    "Choose the layers, starting from the top of the model, to train. Since medical images are very different from natural images, we will need to leave more layers enabled for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all layers by default\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Select all layers to train\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Collect the trainable status of each layer\n",
    "data = []\n",
    "for layer in model.layers:\n",
    "     data.append([str(layer), str(layer.trainable)])\n",
    "\n",
    "# Print the trainable status in nice columns\n",
    "col_width = max(len(word) for row in data for word in row) + 2 \n",
    "for row in data:\n",
    "    row[0] = row[0].ljust(col_width, \".\")\n",
    "    # Uncomment to print trainable status\n",
    "    #print(\"\\t\".join(word.ljust(col_width, \" \") for word in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model**\n",
    "\n",
    "We use binary cross entropy because we have a 2-class classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr = learning_rate)\n",
    "model.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up callbacks**\n",
    "\n",
    "We define an accuracy callback to track the validation loss and validation accuracy at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy callback\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkpoint path may need to be modified if you plan to run this notebook on another machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose checkpoint path\n",
    "checkpoint_path = \"./ResNet_Binary_Classification\"\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Create checkpointer to save best model weights\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path + \"/weights.h5\", verbose=1, monitor=\"val_loss\", mode=\"min\", save_best_only=True)\n",
    "        \n",
    "# Create accuracy callback\n",
    "history = AccuracyHistory()\n",
    "\n",
    "# TEST: ReduceLROnPlateau\n",
    "callback_list = [checkpointer, history, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 3.,\n",
    "                1: 1.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2099 samples, validate on 599 samples\n",
      "Epoch 1/100\n",
      " - 71s - loss: 0.4112 - acc: 0.8742 - val_loss: 0.1960 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19595, saving model to ./ResNet_Binary_Classification/weights.h5\n",
      "Epoch 2/100\n",
      " - 68s - loss: 0.2843 - acc: 0.9219 - val_loss: 0.1665 - val_acc: 0.9366\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19595 to 0.16648, saving model to ./ResNet_Binary_Classification/weights.h5\n",
      "Epoch 3/100\n",
      " - 68s - loss: 0.2277 - acc: 0.9419 - val_loss: 0.2023 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16648\n",
      "Epoch 4/100\n",
      " - 69s - loss: 0.1586 - acc: 0.9581 - val_loss: 0.5127 - val_acc: 0.8314\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16648\n",
      "Epoch 5/100\n",
      " - 69s - loss: 0.1685 - acc: 0.9605 - val_loss: 0.1767 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16648\n",
      "Epoch 6/100\n",
      " - 69s - loss: 0.1053 - acc: 0.9733 - val_loss: 0.2413 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16648\n",
      "Epoch 7/100\n",
      " - 69s - loss: 0.0954 - acc: 0.9786 - val_loss: 0.2975 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16648\n",
      "Epoch 8/100\n",
      " - 69s - loss: 0.0801 - acc: 0.9771 - val_loss: 0.2975 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16648\n",
      "Epoch 9/100\n",
      " - 69s - loss: 0.1043 - acc: 0.9705 - val_loss: 0.3205 - val_acc: 0.8314\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16648\n",
      "Epoch 10/100\n",
      " - 69s - loss: 0.0675 - acc: 0.9838 - val_loss: 0.2106 - val_acc: 0.9499\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.16648\n",
      "Epoch 11/100\n",
      " - 69s - loss: 0.0704 - acc: 0.9833 - val_loss: 0.2693 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.16648\n",
      "Epoch 12/100\n",
      " - 69s - loss: 0.0528 - acc: 0.9838 - val_loss: 0.5377 - val_acc: 0.8932\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47c90b4588>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history = model.fit(X_train, \n",
    "          Y_train, \n",
    "          batch_size=4, \n",
    "          epochs=100, \n",
    "          verbose=2, \n",
    "          class_weight=class_weight,\n",
    "          validation_data=(X_val, Y_val), \n",
    "          callbacks=callback_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,len(history.acc)+1), history.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load best weights from checkpointer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path + \"/weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score the model with our test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.12250685563119147\n",
      "Test accuracy: 0.9601328905634706\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Predictions**\n",
    "\n",
    "Generates predictions for `display_count` number of sample images from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_count = 6\n",
    "\n",
    "# Randomly choose display_count samples\n",
    "indices = np.random.choice(len(X_test), display_count)\n",
    "X_display = X_test[indices]\n",
    "Y_display = Y_test[indices]\n",
    "\n",
    "Pred_display = []\n",
    "for x in X_display:\n",
    "    x = np.reshape(x, (1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "    Pred_display.append(np.argmax(model.predict(x).tolist()[0]))\n",
    "Y_display = [np.argmax(y) for y in Y_display]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Predictions**\n",
    "\n",
    "The following code displays a set of predictions (up to 16) and their associated images. If the prediction is wrong, it is highlighted in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = math.floor((display_count**.5))\n",
    "columns = math.ceil(display_count / rows)\n",
    "\n",
    "# Create the subplots\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(16,16*(rows/columns)))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.25, hspace=0.25)\n",
    "\n",
    "# Format each subplot\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        index = (i * columns) + j\n",
    "        if index >= len(X_display):\n",
    "            continue # If the index is out of bounds, skip this plot\n",
    "        ax[i][j].imshow(X_display[index]) # Display the image\n",
    "        \n",
    "        # If the prediction is wrong, display in red. Otherwise, display in black\n",
    "        color = (0,0,0,1) if Pred_display[index] == Y_display[index] else (0.8,0.1,0.1,1)\n",
    "        \n",
    "        # Display the prediction above the image\n",
    "        title = \"No Line\" if Pred_display[index] == 1 else \"Line\"\n",
    "        ax[i][j].set_title(\"{}: {}\".format(index, title), color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine one of the images more closely, use the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "fig.tight_layout()\n",
    "plt.imshow(X_display[image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the keras session, freeing up GPU\n",
    "keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
