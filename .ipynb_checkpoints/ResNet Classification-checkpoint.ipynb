{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Detection in Chest Radiographs\n",
    "\n",
    "The goal of this notebook is to use pretrained ImageNet models and transfer learning to identify lines in . medical images. These lines can be catheters, tubes, or many other types of medical support devices. We are using the [Stanford CheXpert dataset](https://stanfordmlgroup.github.io/competitions/chexpert/) for training and validation. \n",
    "\n",
    "This is a binary classification problem. The network will simply determine if an X-ray image contains or does not contain any medical lines with the classes `Line` and `NoLine` respectively.\n",
    "\n",
    "The end goal is to be able to use the weights in this network as a backbone for a more complex medical image segmentation network such as U-Net or Mask R-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 22:11:17) \n",
      "[GCC 7.3.0]\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import keras\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "print(sys.version)\n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "The names of the images with lines, and those without lines, are stored in a two separate text files. We will need to read these text files, load the images, then split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input image size\n",
    "input_size = (512, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the image list files**\n",
    "\n",
    "Load the file containing paths to the line images, and the file containing paths to the noline images. (Loading images may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/ryan_data/CheXpert/CheXpert-v1.0/train/\" # Path to CheXpert dataset train/val folder \n",
    "line_path = \"./line.txt\"\n",
    "noline_path = \"./noline.txt\"\n",
    "\n",
    "def load_image_cv(path):\n",
    "    \"\"\"Load grayscale image from path\"\"\"\n",
    "    return cv2.imread(path)\n",
    "\n",
    "def load_samples(file):\n",
    "    images = []\n",
    "    for image_path in file:\n",
    "        path = os.path.join(base_path, image_path.rstrip())\n",
    "        images.append(cv2.resize(load_image_cv(path), input_size))\n",
    "    return np.asarray(images)\n",
    "\n",
    "line_file = open(line_path, mode=\"r\", encoding=\"utf-8\")\n",
    "line_images = load_samples(line_file)\n",
    "\n",
    "noline_file = open(noline_path, mode=\"r\", encoding=\"utf-8\")\n",
    "noline_images = load_samples(noline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an image to check that images were loaded correctly\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 14))\n",
    "ax[0].imshow(line_images[11], cmap=\"gray\")\n",
    "ax[0].set_title(\"Line\")\n",
    "ax[1].imshow(noline_images[11], cmap=\"gray\")\n",
    "ax[1].set_title(\"Noline\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of line images: {}\".format(len(line_images)))\n",
    "print(\"Number of noline images: {}\".format(len(noline_images)))\n",
    "print(\"Shape of each line sample: {}\".format(line_images[0].shape))\n",
    "print(\"Shape of each noline sample: {}\".format(noline_images[0].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine Data and Labels**\n",
    "\n",
    "Since our data is intially split into separate lists by class, we need to rearrange them into two ordered lists of sample images and their labels. We use the following binary labels:\n",
    "* 0 = noline\n",
    "* 1 = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, some methods may be included to improve the class split\n",
    "total_samples = len(line_images) + len(noline_images)\n",
    "line_percent = len(line_images) / total_samples * 100\n",
    "noline_percent = len(noline_images) / total_samples * 100\n",
    "print(\"Class split: {0: .2f}% line, {1: .2f}% noline\".format(line_percent, noline_percent))\n",
    "\n",
    "# Combine data into samples and labels arrays\n",
    "samples = np.asarray([x for x in line_images] + [x for x in noline_images])\n",
    "labels = np.asarray([(1,0) for x in line_images] + [(0,1) for x in noline_images])\n",
    "\n",
    "samples = np.reshape(samples, (len(samples), input_size[0], input_size[1], 3))\n",
    "\n",
    "# Print sample and label shapes\n",
    "print(\"Samples shape: {}\".format(samples.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train/test/val sets**\n",
    "\n",
    "Split the data into individual sets. Here we use a 70/20/10 split for training, validation, and testing respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize dataset ordering\n",
    "ordering = np.arange(len(samples))\n",
    "np.random.shuffle(ordering)\n",
    "samples = samples[ordering]\n",
    "labels = labels[ordering]\n",
    "\n",
    "\n",
    "train_split = 0.7\n",
    "val_split = 0.2\n",
    "test_split = 0.1\n",
    "n = len(samples)\n",
    "\n",
    "# Split data\n",
    "train_cutoff = int(n * train_split)\n",
    "val_cutoff = int(train_cutoff + (n * val_split))\n",
    "X_train, X_val, X_test = samples[:train_cutoff], samples[train_cutoff:val_cutoff], samples[val_cutoff:]\n",
    "Y_train, Y_val, Y_test = labels[:train_cutoff], labels[train_cutoff:val_cutoff], labels[val_cutoff:]\n",
    "\n",
    "print(\"Train length:\\t{}\\nTest Length:\\t{}\\nVal Length:\\t{}\".format(len(X_train), len(X_val), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.01\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create base model**\n",
    "\n",
    "We initialize the ResNet50 model, removing the final classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ryansullivan/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create the ResNet50 base model Dense(1000) layers\n",
    "# Docs: https://keras.rstudio.com/reference/application_resnet50.html\n",
    "resnet_base = ResNet50(weights=\"imagenet\", input_shape=(input_size[0], input_size[1], 3), include_top=False, pooling=\"avg\")\n",
    "\n",
    "# Uncomment for the base model summary\n",
    "#resnet_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add classification layers to ResNet**\n",
    "\n",
    "We add two dense layers and an output layer to provide some room for the network to map CNN features to our outputs. Since this is a binary classification problem, we will use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras functional programming syntax: https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "# Add 2 dense layers and a final prediction layer\n",
    "top = Dense(1024, activation=\"relu\")(resnet_base.output) # TEST: kernel_regularizer = regularizers.l2(0.05)\n",
    "top = Dense(256, activation=\"relu\")(top)\n",
    "predictions = Dense(2, activation=\"softmax\")(top)\n",
    "model = Model(inputs=resnet_base.input, outputs=predictions)\n",
    "\n",
    "# Uncomment for full model summary\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose the trainable layers**\n",
    "\n",
    "Choose the layers, starting from the top of the model, to train. Since medical images are very different from natural images, we will need to leave more layers enabled for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all layers by default\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Select all layers to train\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Collect the trainable status of each layer\n",
    "data = []\n",
    "for layer in model.layers:\n",
    "     data.append([str(layer), str(layer.trainable)])\n",
    "\n",
    "# Print the trainable status in nice columns\n",
    "col_width = max(len(word) for row in data for word in row) + 2 \n",
    "for row in data:\n",
    "    row[0] = row[0].ljust(col_width, \".\")\n",
    "    # Uncomment to print trainable status\n",
    "    #print(\"\\t\".join(word.ljust(col_width, \" \") for word in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model**\n",
    "\n",
    "We use binary cross entropy because we have a 2-class classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr = learning_rate)\n",
    "model.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up callbacks**\n",
    "\n",
    "We define an accuracy callback to track the validation loss and validation accuracy at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy callback\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkpoint path may need to be modified if you plan to run this notebook on another machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose checkpoint path\n",
    "checkpoint_path = \"./ResNet_Binary_Classification\"\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "\n",
    "# Create checkpointer to save best model weights\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path + \"/weights.h5\", verbose=1, monitor=\"val_loss\", mode=\"min\", save_best_only=True)\n",
    "        \n",
    "# Create accuracy callback\n",
    "history = AccuracyHistory()\n",
    "\n",
    "# TEST: ReduceLROnPlateau\n",
    "callback_list = [checkpointer, history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 3.,\n",
    "                1: 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, \n",
    "          Y_train, \n",
    "          batch_size=1, \n",
    "          epochs=100, \n",
    "          verbose=2, \n",
    "          class_weight=class_weight,\n",
    "          validation_data=(X_val, Y_val), \n",
    "          callbacks=callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(history.acc)), history.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load best weights from checkpointer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path + \"/weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score the model with our test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_count = 8\n",
    "index = random.randint(0, len(X_test) - display_count)\n",
    "X_display = X_test[index:index+display_count]\n",
    "Y_display = Y_test[index:index+display_count]\n",
    "Pred_display = []\n",
    "for x in X_display:\n",
    "    x = np.reshape(x, (1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "    Pred_display.append(np.argmax(model.predict(x).tolist()[0]))\n",
    "Y_display = [np.argmax(y) for y in Y_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = math.floor((display_count**.5))\n",
    "columns = math.ceil(display_count / rows)\n",
    "\n",
    "# Create the subplots\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(16,16*(columns/(columns+rows))))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.25, hspace=0.25)\n",
    "\n",
    "# Format each subplot\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        index = (i * columns) + j\n",
    "        if index >= len(X_display):\n",
    "            continue # If the index is out of bounds, skip this plot\n",
    "        ax[i][j].imshow(X_display[index]) # Display the image\n",
    "        \n",
    "        # If the prediction is wrong, display in red. Otherwise, display in black\n",
    "        color = (0,0,0,1) if Pred_display[index] == Y_display[index] else (0.8,0.1,0.1,1)\n",
    "        \n",
    "        # Display the prediction above the image\n",
    "        title = \"No Line\" if Pred_display[index] == 1 else \"Line\"\n",
    "        ax[i][j].set_title(title, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
